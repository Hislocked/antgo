# -*- coding: UTF-8 -*-
# Time: 1/6/18
# File: yesno_crowdsource.py
# Author: jian<jian@mltalker.com>
from __future__ import division
from __future__ import unicode_literals
from __future__ import print_function
from antgo.measures.crowdsource import *
from bs4 import BeautifulSoup


class AntYesNoCrowdsource(AntCrowdsource):
  def __init__(self, task, name):
    super(AntYesNoCrowdsource, self).__init__(task, name)

    # set _client_response_data
    self.client_response_data = {'RESPONSE': ['LEFT', 'RIGHT'], 'TYPE': 'SELECT'}

    # set _client_query_html
    self.client_query_html = ''

    # set _client_query_js
    self.client_query_js = ''

    # set _client_query_data
    self.client_query_data = {'QUERY': {'PREDICT_PREDICT': 'IMAGE', 'GROUNDTRUTH_GT': 'IMAGE'}}
    
    self.is_random_layout = True
    self.crowdsource_helper = 'Please select which is real image. The one is real image and the other one is fake (generated by AI).'
    
    self._is_support_rank = True
  
  def where_in_table(self, worksite, element_id):
    if type(element_id) != list:
      element_id = [element_id]

    output = []
    soup = BeautifulSoup(worksite, 'lxml')
    for element in element_id:
      element_node = soup.find(id=element)
      container_id = element_node.parent.get('id')
      xy = container_id.split('-')[-1]
      output.append(xy)

    return output

  def prepare_custom_ground_truth_response(self, client_id, query_index, record_db):
    # CONCLUSION, WORKSITE
    user_worksite = self._client_response_record[client_id]['RESPONSE'][query_index]['WORKSITE']

    # 'PREDICT','GT' position in table
    xy = self.where_in_table(user_worksite, ['PREDICT', 'GT'])

    user_gt_response = ''
    if int(xy[0][1]) < int(xy[1][1]):
      user_gt_response = 'RIGHT is REAL'
    else:
      user_gt_response = 'LEFT is REAL'

    # parse user_worksite (html)
    return {'CUSTOM_GT': {'DATA': user_gt_response, 'TYPE': 'TEXT'}}

  def eva(self, data=None, label=None):
    if len(self._client_response_record) == 0:
      with open(os.path.join(self.dump_dir, 'crowdsource_record.txt')) as fp:
        content = fp.read()
        self._client_response_record = json.loads(content)
    
    score = None
    person_count = 0
    for _, client_response in self._client_response_record.items():
      response_data = client_response['RESPONSE']
      query_index_map = client_response['ID']
      
      if score is None:
        score = np.zeros((len(query_index_map), len(self._client_response_record)))
      
      for query_index, client_data in enumerate(response_data):
        real_query_index = query_index_map[query_index]
        client_worksite = client_data['WORKSITE']
        client_conclusion = client_data['CONCLUSION']
        
        xy = self.where_in_table(client_worksite, ['PREDICT', 'GT'])
        if int(xy[0][1]) < int(xy[1][1]):
          if client_conclusion == 'RIGHT':
            score[real_query_index, person_count] = 1.0
        else:
          if client_conclusion == 'LEFT':
            score[real_query_index, person_count] = 1.0

      person_count += 1
      
    evaluation_score = np.mean(score)
    return {'statistic': {'name': self.name,
                          'value': [{'name': self.name, 'value': float(evaluation_score), 'type': 'SCALAR'}]}, }